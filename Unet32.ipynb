{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import cv2 as cv\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMG_PATH = 'train/image'\n",
    "TRAIN_LABEL_PATH = 'train/label'\n",
    "VAL_IMG_PATH = 'val/image'\n",
    "VAL_LABEL_PATH = 'val/label'\n",
    "BATCHSIZE = 32\n",
    "IMAGESIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/ivanshingel/cars-segmentation-research\n",
    "class CityscapesDataset(Dataset):\n",
    "    def __init__(self, transforms= None, train= True):\n",
    "        self.train = train\n",
    "        self.images_path = TRAIN_IMG_PATH\n",
    "        self.labels_path = TRAIN_LABEL_PATH\n",
    "        if not train:\n",
    "            self.images_path = VAL_IMG_PATH\n",
    "            self.labels_path = VAL_LABEL_PATH\n",
    "\n",
    "        self.len = len(os.listdir(self.images_path))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #load sample {img,,label}\n",
    "        naming_label = int(os.listdir(self.images_path)[index].split('.')[0])\n",
    "        image = np.asarray(np.load(os.path.join(self.images_path, f'{os.listdir(self.images_path)[index]}')),\n",
    "                                            dtype= np.float32)\n",
    "        image = cv.resize(image, (IMAGESIZE, IMAGESIZE), interpolation = cv.INTER_NEAREST)\n",
    "        image = torch.from_numpy(image)\n",
    "        image = image.permute(2, 0, 1)\n",
    "\n",
    "        labelSuffix = '_label'\n",
    "        label = np.asarray(np.load(os.path.join(self.labels_path, f'{naming_label}{labelSuffix}.npy')))\n",
    "\n",
    "        label = cv.resize(label, (IMAGESIZE, IMAGESIZE), interpolation = cv.INTER_NEAREST)\n",
    "        label = torch.from_numpy(label)\n",
    "        label = torch.Tensor(label)\n",
    "        label = label.reshape(1, IMAGESIZE, IMAGESIZE)\n",
    "        label = (label + 1)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img_and_mask(img, label, epoch = None):\n",
    "    if epoch != None:\n",
    "        fig = plt.figure(figsize=(10, 7))\n",
    "        fig.add_subplot(1, 2, 1)\n",
    "        img = img.permute(1,2,0).detach().numpy()\n",
    "        plt.imshow(img * IMAGESIZE)\n",
    "        fig.add_subplot(1, 2, 2)\n",
    "        plt.imshow(label.permute(1,2,0).detach().numpy())\n",
    "        plt.savefig('pic/epoch_'+str(epoch)+'.png')\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(10, 7))\n",
    "        fig.add_subplot(1, 2, 1)\n",
    "        img = img.permute(1,2,0)\n",
    "        plt.imshow(img * IMAGESIZE)\n",
    "        fig.add_subplot(1, 2, 2)\n",
    "        plt.imshow(label.permute(1,2,0))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = CityscapesDataset(train=True)\n",
    "validation_set = CityscapesDataset(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=BATCHSIZE, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=BATCHSIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(torch.nn.Module):\n",
    "    \"\"\"Convelution layer\n",
    "       convelution layers for one step on the \"stairs\"\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.convlayer = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias = False),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias = False),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convlayer(x)\n",
    "\n",
    "class Down(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Makes the step down\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.double_conv = ConvLayer(in_channels, out_channels)\n",
    "        self.down_sample = torch.nn.MaxPool2d((2,2), stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_out = self.double_conv(x)\n",
    "        down_out = self.down_sample(skip_out)\n",
    "        return (down_out, skip_out)\n",
    "\n",
    "class Up(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    makes the step up\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Up, self).__init__()\n",
    "\n",
    "        self.double_conv = ConvLayer(in_channels, out_channels)\n",
    "        self.up_sample = torch.nn.ConvTranspose2d(in_channels= out_channels, out_channels= out_channels, kernel_size=(2,2), stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.double_conv(x)\n",
    "        return self.up_sample(x)\n",
    "\n",
    "class Unet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet, self).__init__()\n",
    "        self.down1 = Down(in_channels = 3, out_channels = 32)\n",
    "        self.down2 = Down(in_channels = 32, out_channels = 64)\n",
    "        self.down3 = Down(in_channels = 64, out_channels = 128)\n",
    "        self.down4 = Down(in_channels = 128, out_channels = 256)\n",
    "\n",
    "        #The bottom step\n",
    "        self.bottom = Up(in_channels = 256, out_channels = 512)\n",
    "\n",
    "        #Up + down 4\n",
    "        self.up4 = Up(in_channels = 512+256, out_channels = 256)\n",
    "        #Up + down 3\n",
    "        self.up3 = Up(in_channels = 256+128, out_channels = 128)\n",
    "        #Up + down 2\n",
    "        self.up2 = Up(in_channels = 128+64, out_channels = 64)\n",
    "        #up and out\n",
    "        self.out1 = ConvLayer(in_channels = 64+32, out_channels = 32)\n",
    "\n",
    "        self.out2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 20, kernel_size=1, bias = False),\n",
    "            torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Down\n",
    "        x, skip1 = self.down1(x)\n",
    "        x, skip2 = self.down2(x)\n",
    "        x, skip3 = self.down3(x)\n",
    "        x, skip4 = self.down4(x)\n",
    "        #Bottom\n",
    "        x = self.bottom(x)\n",
    "        #Up\n",
    "        x = torch.cat((skip4, x), dim = 1)\n",
    "        x = self.up4(x)\n",
    "\n",
    "        x = torch.cat((skip3, x), dim = 1)\n",
    "        x = self.up3(x)\n",
    "\n",
    "        x = torch.cat((skip2, x), dim = 1)\n",
    "        x = self.up2(x)\n",
    "\n",
    "        x = torch.cat((skip1, x), dim = 1)\n",
    "        x = self.out1(x)\n",
    "        x = self.out2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input size\n",
    "model = Unet()\n",
    "summary(model, input_size = (3,128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for b, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "        # Compute the loss and its gradients\n",
    "        batch = len(outputs)\n",
    "        labels = labels.long()\n",
    "\n",
    "        loss = loss_fn(outputs.reshape(batch, 20, IMAGESIZE, IMAGESIZE),\n",
    "                       labels.reshape(batch, IMAGESIZE, IMAGESIZE))\n",
    "        loss.backward()\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if b % 10 == 9:\n",
    "            last_loss = running_loss / b # loss per batch\n",
    "            #print('  batch {} loss: {}'.format(b + 1, last_loss))\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "best_vloss = 100_000\n",
    "\n",
    "timeLine = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = datetime.now()\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            #Send to GPU\n",
    "            vinputs = vinputs.to(device)\n",
    "            vlabels = vlabels.to(device)\n",
    "\n",
    "            voutputs = model(vinputs)\n",
    "            batch = len(voutputs)\n",
    "            vlabels = vlabels.long()\n",
    "\n",
    "            \"\"\"\n",
    "            vloss = 0\n",
    "            for e in range(len(voutputs)):\n",
    "                    #if i == 1 and e == 1:\n",
    "                        #show_img_and_mask(vinputs[e], voutputs[e], epoch=epoch)\n",
    "                    temploss = loss_fn(voutputs[e], vlabels[e])/(IMAGESIZE*IMAGESIZE)\n",
    "                    vloss += temploss\n",
    "            running_vloss += vloss/BATCHSIZE\"\"\"\n",
    "            vloss = loss_fn(voutputs.reshape(batch, 20, IMAGESIZE, IMAGESIZE),\n",
    "                            vlabels.reshape(batch, IMAGESIZE, IMAGESIZE))\n",
    "            running_vloss += vloss\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {} time {}'.format(avg_loss, avg_vloss, datetime.now()-start ))\n",
    "    timeLine.append([epoch, avg_loss, avg_vloss.item()])\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss or epoch_number + 1 == EPOCHS:\n",
    "        print('model saved')\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'models/model_{}.pt'.format(epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    elif epoch_number%10 == 9:\n",
    "        print('model saved periodically')\n",
    "        model_path = 'models/model_{}.pt'.format(epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    pd.DataFrame(timeLine, columns= ['Epoch', 'Train Loss', 'Validation Loss']).to_csv('loss.csv')\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = 0,0\n",
    "\n",
    "for b, data in enumerate(training_loader):\n",
    "    if b == 1:\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet()\n",
    "models = [f for f in os.listdir('models')]\n",
    "\n",
    "for weights in models:\n",
    "    epoch = int(weights.split('_')[1][:-3])+1\n",
    "    model = Unet()\n",
    "    model.load_state_dict(torch.load('models/{}'.format(weights),map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "\n",
    "    out = model(inputs)\n",
    "\n",
    "    #make picture\n",
    "    show_img_and_mask(out[28], labels[28], epoch = epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
